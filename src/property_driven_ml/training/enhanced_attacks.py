"""
Enhanced attack implementations that work with unified input region constraints.

This module provides attack classes that can extract input bounds from
InputRegionConstraint objects, eliminating the need for BoundedDataset classes.
"""

from __future__ import print_function

from typing import Tuple

from abc import ABC, abstractmethod

import torch

from ..logics.logic import Logic
from ..constraints.constraints import Constraint
from ..constraints.unified_constraints import InputRegionConstraint


class EnhancedAttack(ABC):
    """
    Enhanced abstract base class for adversarial attack methods.

    This version can work with InputRegionConstraint objects to extract
    bounds dynamically, supporting the unified constraint architecture.

    Args:
        x0: Initial input tensor for attack initialization.
        logic: Logic framework for constraint evaluation.
        device: PyTorch device for tensor computations.
        steps: Number of attack steps to perform.
        restarts: Number of random restarts for attack.
        mean: Data normalization mean values.
        std: Data normalization standard deviation values.
    """

    def __init__(
        self,
        x0: torch.Tensor,
        logic: Logic,
        device: torch.device,
        steps: int,
        restarts: int,
        mean: torch.Tensor | Tuple[float, ...] = (0.0,),
        std: torch.Tensor | Tuple[float, ...] = (1.0,),
        transform_pipeline: None | torch.nn.Sequential = None,
    ):
        self.logic = logic
        self.device = device
        self.steps = steps
        self.restarts = restarts
        self.mean = torch.as_tensor(mean, device=device)
        self.std = torch.as_tensor(std, device=device)

        self.ndim = x0.ndim

        def expand(tensor: torch.Tensor) -> torch.Tensor:
            return tensor.view(*tensor.shape, *([1] * (self.ndim - tensor.ndim)))

        self.expand_mean, self.expand_std = expand(self.mean), expand(self.std)

        if transform_pipeline is not None:
            self.min, self.max = torch.tensor(0.0), torch.tensor(1.0)
        else:
            self.min, self.max = (
                (torch.tensor(0.0) - self.expand_mean) / self.expand_std,
                (torch.tensor(1.0) - self.expand_mean) / self.expand_std,
            )

    def attack_enhanced(
        self,
        N: torch.nn.Module,
        x: torch.Tensor,
        y: torch.Tensor,
        constraint: InputRegionConstraint,
    ) -> torch.Tensor:
        """
        Enhanced attack method that gets bounds from InputRegionConstraint.

        Args:
            N: Neural network model to attack.
            x: Input tensor for the attack.
            y: Target labels for the attack.
            constraint: InputRegionConstraint that can provide bounds.

        Returns:
            Adversarial examples generated by the attack.
        """
        # Get bounds dynamically from the constraint
        lo, hi = constraint.get_input_bounds(x)
        # Call the original attack method with the extracted bounds
        return self.attack(N, x, y, (lo, hi), constraint)

    @abstractmethod
    def attack(
        self,
        N: torch.nn.Module,
        x: torch.Tensor,
        y: torch.Tensor,
        bounds: Tuple[torch.Tensor, torch.Tensor],
        constraint: Constraint,
    ) -> torch.Tensor:
        """
        Abstract method for generating adversarial examples using the specified attack strategy.

        Args:
            N: Neural network model to attack.
            x: Input tensor for the attack.
            y: Target labels for the attack.
            bounds: Tuple of (lower_bound, upper_bound) for input constraints.
            constraint: Constraint to enforce during the attack.

        Returns:
            Adversarial examples generated by the attack.
        """
        pass

    def uniform_random_sample(
        self, bounds: Tuple[torch.Tensor, torch.Tensor]
    ) -> torch.Tensor:
        """Generate uniform random samples within the given bounds."""
        lo, hi = bounds
        return torch.rand_like(lo) * (hi - lo) + lo

    def uniform_random_sample_enhanced(
        self, x: torch.Tensor, constraint: InputRegionConstraint
    ) -> torch.Tensor:
        """Generate uniform random samples using constraint bounds."""
        lo, hi = constraint.get_input_bounds(x)
        return self.uniform_random_sample((lo, hi))


class EnhancedPGD(EnhancedAttack):
    """
    Enhanced Projected Gradient Descent adversarial attack.

    Implements PGD attack by iteratively applying gradient steps
    and projecting back to the allowed perturbation region.
    Works with InputRegionConstraint for dynamic bound computation.

    Args:
        x0: Initial input tensor for attack initialization.
        logic: Logic framework for constraint evaluation.
        device: PyTorch device for tensor computations.
        steps: Number of PGD steps to perform.
        restarts: Number of random restarts for attack.
        step_size: Step size for gradient updates.
    """

    def __init__(
        self,
        x0: torch.Tensor,
        logic: Logic,
        device: torch.device,
        steps: int,
        restarts: int,
        step_size: float,
        mean: torch.Tensor | Tuple[float, ...] = (0.0,),
        std: torch.Tensor | Tuple[float, ...] = (1.0,),
        transform_pipeline: None | torch.nn.Sequential = None,
    ):
        super().__init__(
            x0, logic, device, steps, restarts, mean, std, transform_pipeline
        )
        self.step_size = step_size

    def attack(
        self,
        N: torch.nn.Module,
        x: torch.Tensor,
        y: torch.Tensor,
        bounds: Tuple[torch.Tensor, torch.Tensor],
        constraint: Constraint,
    ) -> torch.Tensor:
        """
        Perform PGD attack with the given bounds and constraint.

        Args:
            N: Neural network model to attack.
            x: Input tensor for the attack.
            y: Target labels for the attack.
            bounds: Tuple of (lower_bound, upper_bound) for input constraints.
            constraint: Constraint to enforce during the attack.

        Returns:
            Adversarial examples generated by PGD.
        """
        lo, hi = bounds
        batch_size = x.shape[0]

        worst_loss = torch.full((batch_size,), float("-inf"), device=self.device)
        best_adv = x.clone()

        for _ in range(self.restarts):
            # Random initialization within bounds
            x_adv = torch.rand_like(x) * (hi - lo) + lo
            x_adv = torch.clamp(x_adv, self.min, self.max)

            for step in range(self.steps):
                x_adv.requires_grad_(True)

                loss, _ = constraint.eval(N, x, x_adv, y, self.logic, reduction=None)

                if x_adv.grad is not None:
                    x_adv.grad.zero_()

                loss.sum().backward()

                with torch.no_grad():
                    # Gradient ascent step
                    if x_adv.grad is not None:
                        x_adv = x_adv + self.step_size * x_adv.grad.sign()

                    # Project to bounds
                    x_adv = torch.clamp(x_adv, lo, hi)
                    x_adv = torch.clamp(x_adv, self.min, self.max)

            # Evaluate final loss for this restart
            with torch.no_grad():
                final_loss, _ = constraint.eval(
                    N, x, x_adv, y, self.logic, reduction=None
                )

                # Keep the worst examples across restarts
                update_mask = final_loss > worst_loss
                worst_loss[update_mask] = final_loss[update_mask]
                best_adv[update_mask] = x_adv[update_mask]

        return best_adv

    def attack_single(
        self,
        N: torch.nn.Module,
        x: torch.Tensor,
        y: torch.Tensor,
        bounds: Tuple[torch.Tensor, torch.Tensor],
        constraint: Constraint,
    ) -> torch.Tensor:
        """
        Perform PGD attack for a single restart.

        Args:
            N: Neural network model to attack.
            x: Input tensor for the attack.
            y: Target labels for the attack.
            bounds: Tuple of (lower_bound, upper_bound) for input constraints.
            constraint: Constraint to enforce during the attack.

        Returns:
            Adversarial examples from single PGD run.
        """
        lo, hi = bounds

        # Random initialization within bounds
        x_adv = torch.rand_like(x) * (hi - lo) + lo
        x_adv = torch.clamp(x_adv, self.min, self.max)

        for step in range(self.steps):
            x_adv.requires_grad_(True)

            loss, _ = constraint.eval(N, x, x_adv, y, self.logic, reduction=None)

            if x_adv.grad is not None:
                x_adv.grad.zero_()

            loss.sum().backward()

            with torch.no_grad():
                # Gradient ascent step
                if x_adv.grad is not None:
                    x_adv = x_adv + self.step_size * x_adv.grad.sign()

                # Project to bounds
                x_adv = torch.clamp(x_adv, lo, hi)
                x_adv = torch.clamp(x_adv, self.min, self.max)

        return x_adv


class EnhancedAPGD(EnhancedAttack):
    """
    Enhanced Auto-PGD adversarial attack.

    Implements the Auto-PGD attack from Croce & Hein (2020) with
    adaptive step size and momentum. Works with InputRegionConstraint
    for dynamic bound computation.

    Args:
        x0: Initial input tensor for attack initialization.
        logic: Logic framework for constraint evaluation.
        device: PyTorch device for tensor computations.
        steps: Number of APGD steps to perform.
        restarts: Number of random restarts for attack.
    """

    def __init__(
        self,
        x0: torch.Tensor,
        logic: Logic,
        device: torch.device,
        steps: int,
        restarts: int,
        mean: torch.Tensor | Tuple[float, ...] = (0.0,),
        std: torch.Tensor | Tuple[float, ...] = (1.0,),
        transform_pipeline: None | torch.nn.Sequential = None,
    ):
        super().__init__(
            x0, logic, device, steps, restarts, mean, std, transform_pipeline
        )

    def attack(
        self,
        N: torch.nn.Module,
        x: torch.Tensor,
        y: torch.Tensor,
        bounds: Tuple[torch.Tensor, torch.Tensor],
        constraint: Constraint,
    ) -> torch.Tensor:
        """
        Perform Auto-PGD attack with the given bounds and constraint.

        Args:
            N: Neural network model to attack.
            x: Input tensor for the attack.
            y: Target labels for the attack.
            bounds: Tuple of (lower_bound, upper_bound) for input constraints.
            constraint: Constraint to enforce during the attack.

        Returns:
            Adversarial examples generated by Auto-PGD.
        """
        lo, hi = bounds
        batch_size = x.shape[0]

        worst_loss = torch.full((batch_size,), float("-inf"), device=self.device)
        best_adv = x.clone()

        for _ in range(self.restarts):
            # Random initialization within bounds
            x_adv = torch.rand_like(x) * (hi - lo) + lo
            x_adv = torch.clamp(x_adv, self.min, self.max)

            # Auto-PGD specific variables
            momentum = torch.zeros_like(x_adv)
            step_size = 2.0 / self.steps

            for step in range(self.steps):
                x_adv.requires_grad_(True)

                loss, _ = constraint.eval(N, x, x_adv, y, self.logic, reduction=None)

                if x_adv.grad is not None:
                    x_adv.grad.zero_()

                loss.sum().backward()

                with torch.no_grad():
                    # Momentum update
                    if x_adv.grad is not None:
                        momentum = 0.75 * momentum + x_adv.grad.sign()

                    # Adaptive step
                    x_adv = x_adv + step_size * momentum

                    # Project to bounds
                    x_adv = torch.clamp(x_adv, lo, hi)
                    x_adv = torch.clamp(x_adv, self.min, self.max)

            # Evaluate final loss for this restart
            with torch.no_grad():
                final_loss, _ = constraint.eval(
                    N, x, x_adv, y, self.logic, reduction=None
                )

                # Keep the worst examples across restarts
                update_mask = final_loss > worst_loss
                worst_loss[update_mask] = final_loss[update_mask]
                best_adv[update_mask] = x_adv[update_mask]

        return best_adv
